{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b46b7df",
   "metadata": {
    "executionInfo": {
     "elapsed": 4198,
     "status": "ok",
     "timestamp": 1624759883403,
     "user": {
      "displayName": "Dima Rekesh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgCxhnQN9cW764WS8AiJguM8wE5foCZLMjr-NyFuQ=s64",
      "userId": "05362657998610812765"
     },
     "user_tz": 420
    },
    "id": "f2513038"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fc45fa8",
   "metadata": {
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1624759889299,
     "user": {
      "displayName": "Dima Rekesh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgCxhnQN9cW764WS8AiJguM8wE5foCZLMjr-NyFuQ=s64",
      "userId": "05362657998610812765"
     },
     "user_tz": 420
    },
    "id": "02d1a0c0"
   },
   "outputs": [],
   "source": [
    "SEED=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edcb151d",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1624759891917,
     "user": {
      "displayName": "Dima Rekesh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgCxhnQN9cW764WS8AiJguM8wE5foCZLMjr-NyFuQ=s64",
      "userId": "05362657998610812765"
     },
     "user_tz": 420
    },
    "id": "b1b9bfde"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a201385",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1624759894660,
     "user": {
      "displayName": "Dima Rekesh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgCxhnQN9cW764WS8AiJguM8wE5foCZLMjr-NyFuQ=s64",
      "userId": "05362657998610812765"
     },
     "user_tz": 420
    },
    "id": "e9eb47a7",
    "outputId": "c49775ff-91ee-488c-d99c-3739e452d6af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2b9736b",
   "metadata": {
    "id": "073b7b81"
   },
   "outputs": [],
   "source": [
    "START_EPOCH = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "468c7d75",
   "metadata": {
    "id": "5e18ae51",
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "ARCH = 'resnet152'\n",
    "EPOCHS = 200\n",
    "LR = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PRINT_FREQ = 50\n",
    "TRAIN_BATCH=256\n",
    "VAL_BATCH=256\n",
    "WORKERS=2\n",
    "TRAINDIR=\"/data/CINIC/train\"\n",
    "VALDIR=\"/data/CINIC/valid\"\n",
    "GPU=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00b4628d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1622949197302,
     "user": {
      "displayName": "Jayanth Srinivasa",
      "photoUrl": "",
      "userId": "03369694624178485882"
     },
     "user_tz": 420
    },
    "id": "c6bf6a83",
    "outputId": "72d2e92f-7574-4c0a-c813-288cd69eaa36"
   },
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print('GPU not detected.. did you pass through your GPU?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f262ab2e",
   "metadata": {
    "id": "acd97390"
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16b5e392",
   "metadata": {
    "id": "e19a5849"
   },
   "outputs": [],
   "source": [
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58b1be05",
   "metadata": {
    "id": "4e65743f"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if GPU is not None:\n",
    "            images = images.cuda(GPU, non_blocking=True)\n",
    "        if torch.cuda.is_available():\n",
    "            target = target.cuda(GPU, non_blocking=True)\n",
    "\n",
    "        # Enables autocasting for the forward pass (model + loss)\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % PRINT_FREQ == 0:\n",
    "            progress.display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2856eca6",
   "metadata": {
    "id": "ab30a1a4"
   },
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            if GPU is not None:\n",
    "                images = images.cuda(GPU, non_blocking=True)\n",
    "            if torch.cuda.is_available():\n",
    "                target = target.cuda(GPU, non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                output = model(images)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % PRINT_FREQ == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1c461d06",
   "metadata": {
    "id": "afa7d9fd"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bae2e2cf",
   "metadata": {
    "id": "8c5f0ab4"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3d92ab69",
   "metadata": {
    "id": "ce30c86a"
   },
   "outputs": [],
   "source": [
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5bfe9c7d",
   "metadata": {
    "id": "7504ce7a"
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = LR * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "04de42a9",
   "metadata": {
    "id": "0d659923"
   },
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "249f5d56",
   "metadata": {
    "id": "f74f06e1"
   },
   "outputs": [],
   "source": [
    "imagenet_mean_RGB = [0.47889522, 0.47227842, 0.43047404]\n",
    "imagenet_std_RGB = [0.229, 0.224, 0.225]\n",
    "cinic_mean_RGB = [0.47889522, 0.47227842, 0.43047404]\n",
    "cinic_std_RGB = [0.24205776, 0.23828046, 0.25874835]\n",
    "cifar_mean_RGB = [0.4914, 0.4822, 0.4465]\n",
    "cifar_std_RGB = [0.2023, 0.1994, 0.2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "554670be",
   "metadata": {
    "id": "c005e2dd"
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=cifar_mean_RGB, std=cifar_std_RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "27ba41eb",
   "metadata": {
    "id": "29d54592"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 32\n",
    "# IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b4b62707",
   "metadata": {
    "id": "94059b7f"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "85297c5a",
   "metadata": {
    "id": "788c0401"
   },
   "outputs": [],
   "source": [
    "model = models.__dict__[ARCH]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bdd681",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "  model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5d5e71fc",
   "metadata": {
    "id": "63dc579e"
   },
   "outputs": [],
   "source": [
    "inf = model.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0ce29ec0",
   "metadata": {
    "id": "edf9cf5d"
   },
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(inf, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ef4ccca1",
   "metadata": {
    "id": "319e2d99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (23): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (24): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (25): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (26): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (27): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (28): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (29): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (30): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (31): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (32): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (33): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (34): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (35): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "457adc74",
   "metadata": {
    "id": "b8dc59b5"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2e743ad5",
   "metadata": {
    "id": "3999d84a"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), LR,\n",
    "                                momentum=MOMENTUM,\n",
    "                                weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bc13c77d",
   "metadata": {
    "id": "9fae338b"
   },
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "44328c3c",
   "metadata": {
    "id": "34dbcdb1"
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cinic_mean_RGB, cinic_std_RGB),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "087a5f89",
   "metadata": {
    "id": "e5275a69"
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(\n",
    "    TRAINDIR, transform=transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d0b3617e",
   "metadata": {
    "id": "854ca1ad"
   },
   "outputs": [],
   "source": [
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cinic_mean_RGB, cinic_std_RGB),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4b3eb655",
   "metadata": {
    "id": "abfa5fb6"
   },
   "outputs": [],
   "source": [
    "val_dataset = datasets.ImageFolder(\n",
    "    VALDIR, transform=transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a30bc22b",
   "metadata": {
    "id": "07a0bdf4"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=TRAIN_BATCH, shuffle=True,\n",
    "        num_workers=WORKERS, pin_memory=True, sampler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9a38f80a",
   "metadata": {
    "id": "192ae835"
   },
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=VAL_BATCH, shuffle=False,\n",
    "        num_workers=WORKERS, pin_memory=True, sampler=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bc0b8261",
   "metadata": {
    "id": "1502c5db"
   },
   "outputs": [],
   "source": [
    "best_acc1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6912e7cc",
   "metadata": {
    "id": "ceb95e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][  0/352]\tTime  1.351 ( 1.351)\tData  0.152 ( 0.152)\tLoss 2.4880e+00 (2.4880e+00)\tAcc@1  10.55 ( 10.55)\tAcc@5  57.42 ( 57.42)\n",
      "Epoch: [0][ 50/352]\tTime  0.072 ( 0.099)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1   9.38 ( 10.20)\tAcc@5  52.73 ( 50.02)\n",
      "Epoch: [0][100/352]\tTime  0.074 ( 0.086)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   8.98 ( 10.25)\tAcc@5  51.95 ( 50.42)\n",
      "Epoch: [0][150/352]\tTime  0.072 ( 0.081)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.94 ( 10.10)\tAcc@5  47.27 ( 50.05)\n",
      "Epoch: [0][200/352]\tTime  0.072 ( 0.079)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.94 ( 10.04)\tAcc@5  48.83 ( 49.94)\n",
      "Epoch: [0][250/352]\tTime  0.072 ( 0.078)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.98 ( 10.06)\tAcc@5  43.36 ( 49.98)\n",
      "Epoch: [0][300/352]\tTime  0.072 ( 0.077)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.98 (  9.99)\tAcc@5  50.39 ( 49.97)\n",
      "Epoch: [0][350/352]\tTime  0.069 ( 0.076)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   6.64 ( 10.02)\tAcc@5  48.05 ( 50.03)\n",
      "Test: [  0/352]\tTime  0.146 ( 0.146)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.055 ( 0.042)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.056 ( 0.041)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.056 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.052 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09999383162408304]\n",
      "Epoch: [1][  0/352]\tTime  0.283 ( 0.283)\tData  0.157 ( 0.157)\tLoss nan (nan)\tAcc@1  10.55 ( 10.55)\tAcc@5  46.88 ( 46.88)\n",
      "Epoch: [1][ 50/352]\tTime  0.072 ( 0.078)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1   8.59 ( 10.27)\tAcc@5  49.22 ( 50.05)\n",
      "Epoch: [1][100/352]\tTime  0.072 ( 0.076)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   9.77 ( 10.15)\tAcc@5  52.73 ( 49.78)\n",
      "Epoch: [1][150/352]\tTime  0.072 ( 0.075)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.16 ( 10.05)\tAcc@5  49.22 ( 49.91)\n",
      "Epoch: [1][200/352]\tTime  0.077 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.98 (  9.99)\tAcc@5  53.52 ( 50.02)\n",
      "Epoch: [1][250/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.98 (  9.94)\tAcc@5  46.88 ( 49.91)\n",
      "Epoch: [1][300/352]\tTime  0.073 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  11.72 (  9.96)\tAcc@5  49.22 ( 50.05)\n",
      "Epoch: [1][350/352]\tTime  0.069 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.20 ( 10.00)\tAcc@5  48.05 ( 50.01)\n",
      "Test: [  0/352]\tTime  0.145 ( 0.145)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.055 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.056 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.056 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.053 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.053 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09997532801828658]\n",
      "Epoch: [2][  0/352]\tTime  0.239 ( 0.239)\tData  0.157 ( 0.157)\tLoss nan (nan)\tAcc@1   9.77 (  9.77)\tAcc@5  44.14 ( 44.14)\n",
      "Epoch: [2][ 50/352]\tTime  0.072 ( 0.076)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1  10.94 ( 10.29)\tAcc@5  54.69 ( 49.54)\n",
      "Epoch: [2][100/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1  12.11 ( 10.14)\tAcc@5  52.34 ( 49.94)\n",
      "Epoch: [2][150/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.16 ( 10.03)\tAcc@5  50.39 ( 49.93)\n",
      "Epoch: [2][200/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   7.42 (  9.98)\tAcc@5  46.88 ( 49.96)\n",
      "Epoch: [2][250/352]\tTime  0.073 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.55 ( 10.04)\tAcc@5  51.17 ( 50.04)\n",
      "Epoch: [2][300/352]\tTime  0.073 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.16 ( 10.07)\tAcc@5  50.39 ( 50.03)\n",
      "Epoch: [2][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  11.33 ( 10.00)\tAcc@5  51.56 ( 50.00)\n",
      "Test: [  0/352]\tTime  0.144 ( 0.144)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.055 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.056 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.056 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.053 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.054 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.053 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09994449374809851]\n",
      "Epoch: [3][  0/352]\tTime  0.257 ( 0.257)\tData  0.160 ( 0.160)\tLoss nan (nan)\tAcc@1  11.33 ( 11.33)\tAcc@5  53.12 ( 53.12)\n",
      "Epoch: [3][ 50/352]\tTime  0.073 ( 0.076)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1   9.77 ( 10.14)\tAcc@5  49.22 ( 50.44)\n",
      "Epoch: [3][100/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   8.98 ( 10.08)\tAcc@5  49.61 ( 50.05)\n",
      "Epoch: [3][150/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.94 ( 10.03)\tAcc@5  58.20 ( 50.04)\n",
      "Epoch: [3][200/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.98 ( 10.01)\tAcc@5  51.17 ( 50.09)\n",
      "Epoch: [3][250/352]\tTime  0.073 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   6.64 (  9.94)\tAcc@5  48.83 ( 49.90)\n",
      "Epoch: [3][300/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   7.42 (  9.97)\tAcc@5  48.44 ( 49.91)\n",
      "Epoch: [3][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  12.50 ( 10.00)\tAcc@5  52.73 ( 50.01)\n",
      "Test: [  0/352]\tTime  0.144 ( 0.144)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.054 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.053 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.053 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.055 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.052 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09990133642141359]\n",
      "Epoch: [4][  0/352]\tTime  0.240 ( 0.240)\tData  0.157 ( 0.157)\tLoss nan (nan)\tAcc@1   8.20 (  8.20)\tAcc@5  46.48 ( 46.48)\n",
      "Epoch: [4][ 50/352]\tTime  0.072 ( 0.076)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1   8.98 ( 10.20)\tAcc@5  53.12 ( 50.21)\n",
      "Epoch: [4][100/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1  12.50 ( 10.21)\tAcc@5  45.70 ( 50.36)\n",
      "Epoch: [4][150/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  13.28 (  9.98)\tAcc@5  49.22 ( 50.19)\n",
      "Epoch: [4][200/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  11.33 ( 10.09)\tAcc@5  48.83 ( 50.14)\n",
      "Epoch: [4][250/352]\tTime  0.073 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.94 ( 10.01)\tAcc@5  50.39 ( 49.99)\n",
      "Epoch: [4][300/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  11.33 ( 10.04)\tAcc@5  57.81 ( 49.99)\n",
      "Epoch: [4][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.94 ( 10.00)\tAcc@5  52.73 ( 49.99)\n",
      "Test: [  0/352]\tTime  0.143 ( 0.143)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.054 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.031 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.039 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.056 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.052 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09984586668665642]\n",
      "Epoch: [5][  0/352]\tTime  0.252 ( 0.252)\tData  0.158 ( 0.158)\tLoss nan (nan)\tAcc@1  10.16 ( 10.16)\tAcc@5  52.73 ( 52.73)\n",
      "Epoch: [5][ 50/352]\tTime  0.072 ( 0.076)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1   9.38 ( 10.13)\tAcc@5  50.00 ( 49.63)\n",
      "Epoch: [5][100/352]\tTime  0.074 ( 0.074)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1  12.11 ( 10.09)\tAcc@5  44.92 ( 49.52)\n",
      "Epoch: [5][150/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.77 ( 10.18)\tAcc@5  52.73 ( 49.66)\n",
      "Epoch: [5][200/352]\tTime  0.073 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.94 ( 10.09)\tAcc@5  53.52 ( 49.80)\n",
      "Epoch: [5][250/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   7.81 (  9.96)\tAcc@5  48.05 ( 49.98)\n",
      "Epoch: [5][300/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.55 (  9.98)\tAcc@5  49.61 ( 50.11)\n",
      "Epoch: [5][350/352]\tTime  0.068 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.38 ( 10.00)\tAcc@5  48.44 ( 50.00)\n",
      "Test: [  0/352]\tTime  0.141 ( 0.141)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.031 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.024 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.024 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.024 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.024 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.024 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.024 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09977809823015402]\n",
      "Epoch: [6][  0/352]\tTime  0.246 ( 0.246)\tData  0.166 ( 0.166)\tLoss nan (nan)\tAcc@1  13.67 ( 13.67)\tAcc@5  54.30 ( 54.30)\n",
      "Epoch: [6][ 50/352]\tTime  0.074 ( 0.077)\tData  0.000 ( 0.004)\tLoss nan (nan)\tAcc@1   7.81 ( 10.09)\tAcc@5  52.73 ( 49.60)\n",
      "Epoch: [6][100/352]\tTime  0.072 ( 0.075)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   9.77 (  9.99)\tAcc@5  56.25 ( 49.81)\n",
      "Epoch: [6][150/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  14.06 ( 10.00)\tAcc@5  55.86 ( 49.89)\n",
      "Epoch: [6][200/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.77 (  9.95)\tAcc@5  49.22 ( 49.98)\n",
      "Epoch: [6][250/352]\tTime  0.073 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.38 ( 10.05)\tAcc@5  49.22 ( 49.89)\n",
      "Epoch: [6][300/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.94 ( 10.01)\tAcc@5  53.12 ( 50.02)\n",
      "Epoch: [6][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.38 ( 10.00)\tAcc@5  46.88 ( 50.01)\n",
      "Test: [  0/352]\tTime  0.150 ( 0.150)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.055 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.038 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.047 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.044 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.052 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09969804777275901]\n",
      "Epoch: [7][  0/352]\tTime  0.238 ( 0.238)\tData  0.158 ( 0.158)\tLoss nan (nan)\tAcc@1   8.59 (  8.59)\tAcc@5  56.64 ( 56.64)\n",
      "Epoch: [7][ 50/352]\tTime  0.073 ( 0.076)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1  12.11 ( 10.06)\tAcc@5  46.88 ( 50.58)\n",
      "Epoch: [7][100/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1  10.16 (  9.92)\tAcc@5  51.56 ( 50.27)\n",
      "Epoch: [7][150/352]\tTime  0.084 ( 0.075)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.77 (  9.72)\tAcc@5  51.17 ( 49.90)\n",
      "Epoch: [7][200/352]\tTime  0.073 ( 0.075)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.38 (  9.78)\tAcc@5  51.56 ( 49.97)\n",
      "Epoch: [7][250/352]\tTime  0.072 ( 0.075)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.77 (  9.87)\tAcc@5  49.61 ( 50.03)\n",
      "Epoch: [7][300/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.77 (  9.93)\tAcc@5  43.36 ( 50.05)\n",
      "Epoch: [7][350/352]\tTime  0.069 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.77 ( 10.00)\tAcc@5  50.39 ( 50.01)\n",
      "Test: [  0/352]\tTime  0.156 ( 0.156)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.055 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.052 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.054 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.051 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09960573506572391]\n",
      "Epoch: [8][  0/352]\tTime  0.239 ( 0.239)\tData  0.158 ( 0.158)\tLoss nan (nan)\tAcc@1   8.98 (  8.98)\tAcc@5  42.58 ( 42.58)\n",
      "Epoch: [8][ 50/352]\tTime  0.072 ( 0.076)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1   7.42 (  9.51)\tAcc@5  51.17 ( 49.83)\n",
      "Epoch: [8][100/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1  13.28 (  9.79)\tAcc@5  49.22 ( 49.80)\n",
      "Epoch: [8][150/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   7.03 (  9.90)\tAcc@5  48.05 ( 49.99)\n",
      "Epoch: [8][200/352]\tTime  0.077 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.55 (  9.96)\tAcc@5  51.17 ( 49.83)\n",
      "Epoch: [8][250/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.16 ( 10.04)\tAcc@5  44.53 ( 49.99)\n",
      "Epoch: [8][300/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.38 ( 10.02)\tAcc@5  49.61 ( 50.08)\n",
      "Epoch: [8][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.38 ( 10.00)\tAcc@5  53.91 ( 50.00)\n",
      "Test: [  0/352]\tTime  0.144 ( 0.144)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.055 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.056 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.053 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.051 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09950118288582789]\n",
      "Epoch: [9][  0/352]\tTime  0.247 ( 0.247)\tData  0.160 ( 0.160)\tLoss nan (nan)\tAcc@1  11.33 ( 11.33)\tAcc@5  48.05 ( 48.05)\n",
      "Epoch: [9][ 50/352]\tTime  0.073 ( 0.076)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1  10.16 ( 10.06)\tAcc@5  48.83 ( 50.26)\n",
      "Epoch: [9][100/352]\tTime  0.072 ( 0.075)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   8.98 (  9.98)\tAcc@5  52.73 ( 50.29)\n",
      "Epoch: [9][150/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  12.11 ( 10.02)\tAcc@5  50.78 ( 50.16)\n",
      "Epoch: [9][200/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.59 ( 10.03)\tAcc@5  46.88 ( 50.10)\n",
      "Epoch: [9][250/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.98 ( 10.01)\tAcc@5  44.92 ( 50.14)\n",
      "Epoch: [9][300/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.77 ( 10.04)\tAcc@5  49.61 ( 50.09)\n",
      "Epoch: [9][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.98 ( 10.00)\tAcc@5  48.83 ( 49.99)\n",
      "Test: [  0/352]\tTime  0.143 ( 0.143)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.046 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.024 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.041 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.031 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.052 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.034 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.024 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09938441702975691]\n",
      "Epoch: [10][  0/352]\tTime  0.240 ( 0.240)\tData  0.159 ( 0.159)\tLoss nan (nan)\tAcc@1   8.20 (  8.20)\tAcc@5  48.83 ( 48.83)\n",
      "Epoch: [10][ 50/352]\tTime  0.072 ( 0.076)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1  10.94 (  9.65)\tAcc@5  53.52 ( 50.13)\n",
      "Epoch: [10][100/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   7.81 (  9.80)\tAcc@5  46.48 ( 50.27)\n",
      "Epoch: [10][150/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  12.89 ( 10.02)\tAcc@5  57.42 ( 50.37)\n",
      "Epoch: [10][200/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   6.64 ( 10.02)\tAcc@5  44.53 ( 50.19)\n",
      "Epoch: [10][250/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.59 (  9.99)\tAcc@5  40.23 ( 50.09)\n",
      "Epoch: [10][300/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   7.42 (  9.98)\tAcc@5  52.73 ( 50.16)\n",
      "Epoch: [10][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  12.11 ( 10.00)\tAcc@5  45.31 ( 50.01)\n",
      "Test: [  0/352]\tTime  0.145 ( 0.145)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.055 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.056 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.053 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09925546630773871]\n",
      "Epoch: [11][  0/352]\tTime  0.239 ( 0.239)\tData  0.158 ( 0.158)\tLoss nan (nan)\tAcc@1   8.20 (  8.20)\tAcc@5  44.53 ( 44.53)\n",
      "Epoch: [11][ 50/352]\tTime  0.072 ( 0.076)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1  11.72 ( 10.00)\tAcc@5  50.78 ( 50.16)\n",
      "Epoch: [11][100/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1  10.16 (  9.97)\tAcc@5  52.34 ( 50.05)\n",
      "Epoch: [11][150/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.59 ( 10.05)\tAcc@5  49.22 ( 50.02)\n",
      "Epoch: [11][200/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  11.33 ( 10.10)\tAcc@5  46.48 ( 50.10)\n",
      "Epoch: [11][250/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  11.33 ( 10.08)\tAcc@5  51.56 ( 49.99)\n",
      "Epoch: [11][300/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   7.81 ( 10.03)\tAcc@5  49.61 ( 50.02)\n",
      "Epoch: [11][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.38 ( 10.00)\tAcc@5  51.95 ( 50.00)\n",
      "Test: [  0/352]\tTime  0.143 ( 0.143)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.054 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.053 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.024 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.024 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.024 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.024 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09911436253643445]\n",
      "Epoch: [12][  0/352]\tTime  0.239 ( 0.239)\tData  0.158 ( 0.158)\tLoss nan (nan)\tAcc@1  14.84 ( 14.84)\tAcc@5  53.12 ( 53.12)\n",
      "Epoch: [12][ 50/352]\tTime  0.073 ( 0.076)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1  11.72 (  9.85)\tAcc@5  54.69 ( 49.72)\n",
      "Epoch: [12][100/352]\tTime  0.072 ( 0.075)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   9.77 (  9.93)\tAcc@5  53.91 ( 50.21)\n",
      "Epoch: [12][150/352]\tTime  0.094 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.98 ( 10.01)\tAcc@5  51.17 ( 50.26)\n",
      "Epoch: [12][200/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  11.72 (  9.94)\tAcc@5  51.17 ( 50.18)\n",
      "Epoch: [12][250/352]\tTime  0.081 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  14.06 (  9.97)\tAcc@5  50.39 ( 50.15)\n",
      "Epoch: [12][300/352]\tTime  0.077 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  12.11 ( 10.04)\tAcc@5  52.73 ( 50.24)\n",
      "Epoch: [12][350/352]\tTime  0.069 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  13.67 ( 10.00)\tAcc@5  51.56 ( 50.01)\n",
      "Test: [  0/352]\tTime  0.144 ( 0.144)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.056 ( 0.042)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.056 ( 0.041)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.056 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.037 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.044 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.045 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.0989611405310883]\n",
      "Epoch: [13][  0/352]\tTime  0.265 ( 0.265)\tData  0.157 ( 0.157)\tLoss nan (nan)\tAcc@1  12.11 ( 12.11)\tAcc@5  48.83 ( 48.83)\n",
      "Epoch: [13][ 50/352]\tTime  0.072 ( 0.077)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1   8.59 ( 10.01)\tAcc@5  50.39 ( 50.38)\n",
      "Epoch: [13][100/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   7.42 (  9.98)\tAcc@5  49.22 ( 50.15)\n",
      "Epoch: [13][150/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.20 (  9.93)\tAcc@5  44.14 ( 50.13)\n",
      "Epoch: [13][200/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.55 (  9.94)\tAcc@5  46.88 ( 50.16)\n",
      "Epoch: [13][250/352]\tTime  0.073 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  13.28 (  9.91)\tAcc@5  52.34 ( 50.14)\n",
      "Epoch: [13][300/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  13.67 (  9.97)\tAcc@5  47.66 ( 50.03)\n",
      "Epoch: [13][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  14.06 ( 10.00)\tAcc@5  50.39 ( 50.00)\n",
      "Test: [  0/352]\tTime  0.156 ( 0.156)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.054 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.043 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.046 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.037 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.032 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.047 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09879583809693739]\n",
      "Epoch: [14][  0/352]\tTime  0.239 ( 0.239)\tData  0.157 ( 0.157)\tLoss nan (nan)\tAcc@1   9.77 (  9.77)\tAcc@5  46.09 ( 46.09)\n",
      "Epoch: [14][ 50/352]\tTime  0.072 ( 0.077)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1   8.20 ( 10.11)\tAcc@5  54.69 ( 50.25)\n",
      "Epoch: [14][100/352]\tTime  0.072 ( 0.075)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   6.64 ( 10.05)\tAcc@5  46.88 ( 50.12)\n",
      "Epoch: [14][150/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.20 ( 10.03)\tAcc@5  56.64 ( 50.03)\n",
      "Epoch: [14][200/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  12.11 ( 10.10)\tAcc@5  53.52 ( 50.10)\n",
      "Epoch: [14][250/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  11.72 ( 10.12)\tAcc@5  51.17 ( 50.18)\n",
      "Epoch: [14][300/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  11.72 ( 10.03)\tAcc@5  48.44 ( 50.09)\n",
      "Epoch: [14][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   5.86 (  9.99)\tAcc@5  49.61 ( 49.99)\n",
      "Test: [  0/352]\tTime  0.143 ( 0.143)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.055 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.056 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.056 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.053 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.051 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09861849601988384]\n",
      "Epoch: [15][  0/352]\tTime  0.240 ( 0.240)\tData  0.160 ( 0.160)\tLoss nan (nan)\tAcc@1   7.42 (  7.42)\tAcc@5  50.00 ( 50.00)\n",
      "Epoch: [15][ 50/352]\tTime  0.072 ( 0.077)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1  11.72 ( 10.16)\tAcc@5  53.12 ( 50.25)\n",
      "Epoch: [15][100/352]\tTime  0.072 ( 0.075)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   8.20 ( 10.11)\tAcc@5  46.09 ( 49.96)\n",
      "Epoch: [15][150/352]\tTime  0.073 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.55 ( 10.20)\tAcc@5  48.83 ( 50.25)\n",
      "Epoch: [15][200/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  11.33 ( 10.14)\tAcc@5  51.95 ( 49.89)\n",
      "Epoch: [15][250/352]\tTime  0.073 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.55 ( 10.09)\tAcc@5  49.22 ( 49.89)\n",
      "Epoch: [15][300/352]\tTime  0.073 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.38 ( 10.02)\tAcc@5  46.48 ( 49.98)\n",
      "Epoch: [15][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.59 ( 10.00)\tAcc@5  50.00 ( 50.00)\n",
      "Test: [  0/352]\tTime  0.144 ( 0.144)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.054 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.056 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.062 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.036 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.024 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.024 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.024 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09842915805643157]\n",
      "Epoch: [16][  0/352]\tTime  0.241 ( 0.241)\tData  0.160 ( 0.160)\tLoss nan (nan)\tAcc@1  10.16 ( 10.16)\tAcc@5  52.34 ( 52.34)\n",
      "Epoch: [16][ 50/352]\tTime  0.072 ( 0.076)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1  11.33 ( 10.39)\tAcc@5  54.69 ( 50.28)\n",
      "Epoch: [16][100/352]\tTime  0.072 ( 0.075)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1  12.11 ( 10.19)\tAcc@5  45.70 ( 49.96)\n",
      "Epoch: [16][150/352]\tTime  0.081 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  12.50 ( 10.08)\tAcc@5  45.70 ( 50.09)\n",
      "Epoch: [16][200/352]\tTime  0.073 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  14.45 ( 10.00)\tAcc@5  50.00 ( 49.91)\n",
      "Epoch: [16][250/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.94 ( 10.00)\tAcc@5  51.95 ( 50.01)\n",
      "Epoch: [16][300/352]\tTime  0.073 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.77 ( 10.04)\tAcc@5  52.34 ( 50.04)\n",
      "Epoch: [16][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.77 ( 10.00)\tAcc@5  50.78 ( 50.01)\n",
      "Test: [  0/352]\tTime  0.154 ( 0.154)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.055 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.057 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.061 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.052 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.049 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09822787092288993]\n",
      "Epoch: [17][  0/352]\tTime  0.255 ( 0.255)\tData  0.157 ( 0.157)\tLoss nan (nan)\tAcc@1  12.50 ( 12.50)\tAcc@5  52.73 ( 52.73)\n",
      "Epoch: [17][ 50/352]\tTime  0.084 ( 0.076)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1  11.33 ( 10.11)\tAcc@5  55.86 ( 50.07)\n",
      "Epoch: [17][100/352]\tTime  0.072 ( 0.075)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1  10.55 ( 10.37)\tAcc@5  53.91 ( 49.95)\n",
      "Epoch: [17][150/352]\tTime  0.073 ( 0.075)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.59 ( 10.14)\tAcc@5  47.66 ( 49.89)\n",
      "Epoch: [17][200/352]\tTime  0.077 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.16 ( 10.05)\tAcc@5  48.05 ( 49.93)\n",
      "Epoch: [17][250/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.55 ( 10.03)\tAcc@5  52.73 ( 49.92)\n",
      "Epoch: [17][300/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.59 (  9.98)\tAcc@5  45.70 ( 49.94)\n",
      "Epoch: [17][350/352]\tTime  0.069 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.55 ( 10.00)\tAcc@5  49.61 ( 50.00)\n",
      "Test: [  0/352]\tTime  0.144 ( 0.144)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.055 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.056 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.052 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.053 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.053 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09801468428384717]\n",
      "Epoch: [18][  0/352]\tTime  0.250 ( 0.250)\tData  0.159 ( 0.159)\tLoss nan (nan)\tAcc@1  12.89 ( 12.89)\tAcc@5  53.12 ( 53.12)\n",
      "Epoch: [18][ 50/352]\tTime  0.072 ( 0.077)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1   8.98 ( 10.12)\tAcc@5  53.52 ( 50.70)\n",
      "Epoch: [18][100/352]\tTime  0.072 ( 0.075)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   8.20 ( 10.04)\tAcc@5  42.58 ( 50.41)\n",
      "Epoch: [18][150/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.59 ( 10.09)\tAcc@5  53.12 ( 50.37)\n",
      "Epoch: [18][200/352]\tTime  0.073 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  13.67 ( 10.24)\tAcc@5  49.61 ( 50.22)\n",
      "Epoch: [18][250/352]\tTime  0.073 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.38 ( 10.08)\tAcc@5  51.95 ( 50.18)\n",
      "Epoch: [18][300/352]\tTime  0.073 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   6.25 ( 10.00)\tAcc@5  51.56 ( 49.93)\n",
      "Epoch: [18][350/352]\tTime  0.069 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   7.42 ( 10.00)\tAcc@5  49.22 ( 49.99)\n",
      "Test: [  0/352]\tTime  0.156 ( 0.156)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.055 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.056 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.053 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.052 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09778965073991652]\n",
      "Epoch: [19][  0/352]\tTime  0.268 ( 0.268)\tData  0.158 ( 0.158)\tLoss nan (nan)\tAcc@1   8.20 (  8.20)\tAcc@5  50.78 ( 50.78)\n",
      "Epoch: [19][ 50/352]\tTime  0.072 ( 0.078)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1   9.77 ( 10.19)\tAcc@5  49.61 ( 50.24)\n",
      "Epoch: [19][100/352]\tTime  0.089 ( 0.076)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1  10.55 (  9.95)\tAcc@5  48.83 ( 50.26)\n",
      "Epoch: [19][150/352]\tTime  0.072 ( 0.075)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.16 ( 10.06)\tAcc@5  46.48 ( 50.27)\n",
      "Epoch: [19][200/352]\tTime  0.072 ( 0.075)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  14.45 ( 10.06)\tAcc@5  52.73 ( 50.14)\n",
      "Epoch: [19][250/352]\tTime  0.073 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.77 ( 10.02)\tAcc@5  47.66 ( 50.00)\n",
      "Epoch: [19][300/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  12.50 ( 10.04)\tAcc@5  48.44 ( 50.05)\n",
      "Epoch: [19][350/352]\tTime  0.069 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   7.81 ( 10.00)\tAcc@5  47.27 ( 49.99)\n",
      "Test: [  0/352]\tTime  0.144 ( 0.144)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.054 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.044 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.042 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.028 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.0975528258147577]\n",
      "Epoch: [20][  0/352]\tTime  0.239 ( 0.239)\tData  0.157 ( 0.157)\tLoss nan (nan)\tAcc@1   9.77 (  9.77)\tAcc@5  50.00 ( 50.00)\n",
      "Epoch: [20][ 50/352]\tTime  0.072 ( 0.076)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1   9.77 (  9.94)\tAcc@5  49.61 ( 50.18)\n",
      "Epoch: [20][100/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   8.98 (  9.82)\tAcc@5  50.39 ( 49.88)\n",
      "Epoch: [20][150/352]\tTime  0.073 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.20 (  9.68)\tAcc@5  47.27 ( 49.88)\n",
      "Epoch: [20][200/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.38 (  9.88)\tAcc@5  46.48 ( 49.88)\n",
      "Epoch: [20][250/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  11.33 (  9.95)\tAcc@5  52.34 ( 49.95)\n",
      "Epoch: [20][300/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.16 (  9.98)\tAcc@5  54.30 ( 50.04)\n",
      "Epoch: [20][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   7.81 ( 10.00)\tAcc@5  47.27 ( 50.00)\n",
      "Test: [  0/352]\tTime  0.144 ( 0.144)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.054 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.052 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.047 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.052 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.042 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09730426794137728]\n",
      "Epoch: [21][  0/352]\tTime  0.276 ( 0.276)\tData  0.156 ( 0.156)\tLoss nan (nan)\tAcc@1   7.03 (  7.03)\tAcc@5  50.00 ( 50.00)\n",
      "Epoch: [21][ 50/352]\tTime  0.072 ( 0.078)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1   8.20 (  9.99)\tAcc@5  49.22 ( 49.76)\n",
      "Epoch: [21][100/352]\tTime  0.072 ( 0.075)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1  11.33 ( 10.10)\tAcc@5  48.44 ( 49.80)\n",
      "Epoch: [21][150/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  12.89 ( 10.15)\tAcc@5  52.73 ( 49.94)\n",
      "Epoch: [21][200/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   7.81 ( 10.02)\tAcc@5  50.00 ( 49.94)\n",
      "Epoch: [21][250/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  14.06 (  9.98)\tAcc@5  48.44 ( 49.97)\n",
      "Epoch: [21][300/352]\tTime  0.088 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.55 (  9.99)\tAcc@5  48.83 ( 49.97)\n",
      "Epoch: [21][350/352]\tTime  0.069 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  13.67 (  9.99)\tAcc@5  52.73 ( 50.00)\n",
      "Test: [  0/352]\tTime  0.152 ( 0.152)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.054 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.049 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.041 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.036 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.045 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.043 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.026 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09704403844771128]\n",
      "Epoch: [22][  0/352]\tTime  0.240 ( 0.240)\tData  0.160 ( 0.160)\tLoss nan (nan)\tAcc@1   7.42 (  7.42)\tAcc@5  50.00 ( 50.00)\n",
      "Epoch: [22][ 50/352]\tTime  0.072 ( 0.076)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1  10.55 (  9.75)\tAcc@5  48.05 ( 49.74)\n",
      "Epoch: [22][100/352]\tTime  0.074 ( 0.074)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   9.77 (  9.88)\tAcc@5  56.64 ( 49.95)\n",
      "Epoch: [22][150/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.20 ( 10.11)\tAcc@5  50.00 ( 50.12)\n",
      "Epoch: [22][200/352]\tTime  0.073 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.98 ( 10.18)\tAcc@5  46.09 ( 50.18)\n",
      "Epoch: [22][250/352]\tTime  0.073 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  12.50 ( 10.09)\tAcc@5  50.00 ( 50.12)\n",
      "Epoch: [22][300/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.98 ( 10.07)\tAcc@5  48.05 ( 50.11)\n",
      "Epoch: [22][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.38 ( 10.00)\tAcc@5  43.36 ( 50.00)\n",
      "Test: [  0/352]\tTime  0.144 ( 0.144)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.055 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.043 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.046 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.052 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09677220154149338]\n",
      "Epoch: [23][  0/352]\tTime  0.239 ( 0.239)\tData  0.157 ( 0.157)\tLoss nan (nan)\tAcc@1  10.94 ( 10.94)\tAcc@5  53.52 ( 53.52)\n",
      "Epoch: [23][ 50/352]\tTime  0.072 ( 0.075)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1  10.16 ( 10.26)\tAcc@5  50.39 ( 49.93)\n",
      "Epoch: [23][100/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   8.20 (  9.96)\tAcc@5  49.61 ( 49.98)\n",
      "Epoch: [23][150/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.59 ( 10.00)\tAcc@5  54.69 ( 50.02)\n",
      "Epoch: [23][200/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.98 (  9.99)\tAcc@5  49.61 ( 49.89)\n",
      "Epoch: [23][250/352]\tTime  0.074 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  11.33 ( 10.03)\tAcc@5  47.66 ( 49.90)\n",
      "Epoch: [23][300/352]\tTime  0.073 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.16 (  9.99)\tAcc@5  47.27 ( 49.91)\n",
      "Epoch: [23][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.16 ( 10.00)\tAcc@5  45.70 ( 49.99)\n",
      "Test: [  0/352]\tTime  0.143 ( 0.143)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.055 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.056 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.053 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.052 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09648882429441258]\n",
      "Epoch: [24][  0/352]\tTime  0.246 ( 0.246)\tData  0.157 ( 0.157)\tLoss nan (nan)\tAcc@1  12.50 ( 12.50)\tAcc@5  50.39 ( 50.39)\n",
      "Epoch: [24][ 50/352]\tTime  0.072 ( 0.076)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1   9.77 ( 10.07)\tAcc@5  51.17 ( 49.73)\n",
      "Epoch: [24][100/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   8.59 ( 10.06)\tAcc@5  48.44 ( 49.96)\n",
      "Epoch: [24][150/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.77 (  9.98)\tAcc@5  47.27 ( 49.90)\n",
      "Epoch: [24][200/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.20 ( 10.10)\tAcc@5  51.56 ( 50.09)\n",
      "Epoch: [24][250/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.16 ( 10.09)\tAcc@5  44.14 ( 50.16)\n",
      "Epoch: [24][300/352]\tTime  0.083 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.20 ( 10.04)\tAcc@5  47.27 ( 50.02)\n",
      "Epoch: [24][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   7.81 ( 10.00)\tAcc@5  54.69 ( 50.00)\n",
      "Test: [  0/352]\tTime  0.155 ( 0.155)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.053 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.035 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.034 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.043 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.052 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09619397662556435]\n",
      "Epoch: [25][  0/352]\tTime  0.252 ( 0.252)\tData  0.161 ( 0.161)\tLoss nan (nan)\tAcc@1   7.42 (  7.42)\tAcc@5  47.27 ( 47.27)\n",
      "Epoch: [25][ 50/352]\tTime  0.072 ( 0.077)\tData  0.000 ( 0.004)\tLoss nan (nan)\tAcc@1  10.16 (  9.93)\tAcc@5  50.78 ( 49.59)\n",
      "Epoch: [25][100/352]\tTime  0.072 ( 0.075)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   8.98 (  9.99)\tAcc@5  51.56 ( 49.91)\n",
      "Epoch: [25][150/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   4.69 ( 10.15)\tAcc@5  42.58 ( 50.04)\n",
      "Epoch: [25][200/352]\tTime  0.073 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  12.11 ( 10.01)\tAcc@5  53.91 ( 49.80)\n",
      "Epoch: [25][250/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   7.81 ( 10.04)\tAcc@5  50.39 ( 49.94)\n",
      "Epoch: [25][300/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.98 ( 10.01)\tAcc@5  43.36 ( 49.92)\n",
      "Epoch: [25][350/352]\tTime  0.069 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.77 ( 10.00)\tAcc@5  47.27 ( 50.01)\n",
      "Test: [  0/352]\tTime  0.144 ( 0.144)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.036 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.025 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.027 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.024 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.024 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.024 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.024 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09588773128419906]\n",
      "Epoch: [26][  0/352]\tTime  0.250 ( 0.250)\tData  0.158 ( 0.158)\tLoss nan (nan)\tAcc@1   9.77 (  9.77)\tAcc@5  47.66 ( 47.66)\n",
      "Epoch: [26][ 50/352]\tTime  0.072 ( 0.077)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1   8.98 ( 10.54)\tAcc@5  50.00 ( 49.60)\n",
      "Epoch: [26][100/352]\tTime  0.072 ( 0.075)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   8.20 ( 10.32)\tAcc@5  54.69 ( 50.15)\n",
      "Epoch: [26][150/352]\tTime  0.073 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.77 ( 10.13)\tAcc@5  48.83 ( 50.18)\n",
      "Epoch: [26][200/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.59 ( 10.19)\tAcc@5  50.78 ( 50.37)\n",
      "Epoch: [26][250/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   6.64 ( 10.10)\tAcc@5  44.53 ( 50.24)\n",
      "Epoch: [26][300/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.59 ( 10.03)\tAcc@5  49.61 ( 50.14)\n",
      "Epoch: [26][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  10.94 ( 10.00)\tAcc@5  51.56 ( 49.99)\n",
      "Test: [  0/352]\tTime  0.144 ( 0.144)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.026 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.028 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.038 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.049 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.032 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.037 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.043 ( 0.039)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09557016383177226]\n",
      "Epoch: [27][  0/352]\tTime  0.254 ( 0.254)\tData  0.174 ( 0.174)\tLoss nan (nan)\tAcc@1  11.72 ( 11.72)\tAcc@5  54.69 ( 54.69)\n",
      "Epoch: [27][ 50/352]\tTime  0.073 ( 0.077)\tData  0.000 ( 0.004)\tLoss nan (nan)\tAcc@1   8.98 ( 10.42)\tAcc@5  50.78 ( 49.86)\n",
      "Epoch: [27][100/352]\tTime  0.072 ( 0.075)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   9.38 ( 10.05)\tAcc@5  51.17 ( 49.87)\n",
      "Epoch: [27][150/352]\tTime  0.073 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  12.50 ( 10.09)\tAcc@5  54.30 ( 49.86)\n",
      "Epoch: [27][200/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  11.72 ( 10.05)\tAcc@5  48.44 ( 50.02)\n",
      "Epoch: [27][250/352]\tTime  0.073 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.98 ( 10.05)\tAcc@5  49.22 ( 49.88)\n",
      "Epoch: [27][300/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.98 ( 10.00)\tAcc@5  48.05 ( 49.91)\n",
      "Epoch: [27][350/352]\tTime  0.069 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.77 (  9.99)\tAcc@5  48.44 ( 49.99)\n",
      "Test: [  0/352]\tTime  0.167 ( 0.167)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.056 ( 0.042)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.057 ( 0.041)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.055 ( 0.041)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.052 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.055 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.048 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09524135262330098]\n",
      "Epoch: [28][  0/352]\tTime  0.240 ( 0.240)\tData  0.158 ( 0.158)\tLoss nan (nan)\tAcc@1  10.94 ( 10.94)\tAcc@5  55.08 ( 55.08)\n",
      "Epoch: [28][ 50/352]\tTime  0.072 ( 0.076)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1   9.77 (  9.44)\tAcc@5  50.78 ( 49.47)\n",
      "Epoch: [28][100/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   9.38 (  9.70)\tAcc@5  52.34 ( 49.64)\n",
      "Epoch: [28][150/352]\tTime  0.073 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  11.33 (  9.74)\tAcc@5  49.61 ( 49.95)\n",
      "Epoch: [28][200/352]\tTime  0.073 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  12.11 (  9.88)\tAcc@5  48.05 ( 49.82)\n",
      "Epoch: [28][250/352]\tTime  0.073 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.38 (  9.93)\tAcc@5  53.52 ( 49.92)\n",
      "Epoch: [28][300/352]\tTime  0.072 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.77 (  9.96)\tAcc@5  54.30 ( 49.96)\n",
      "Epoch: [28][350/352]\tTime  0.069 ( 0.073)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.98 ( 10.00)\tAcc@5  50.78 ( 49.99)\n",
      "Test: [  0/352]\tTime  0.143 ( 0.143)\tLoss nan (nan)\tAcc@1   0.00 (  0.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 50/352]\tTime  0.051 ( 0.041)\tLoss nan (nan)\tAcc@1 100.00 ( 31.07)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/352]\tTime  0.057 ( 0.041)\tLoss nan (nan)\tAcc@1   0.00 ( 34.81)\tAcc@5 100.00 (100.00)\n",
      "Test: [150/352]\tTime  0.057 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 23.28)\tAcc@5 100.00 (100.00)\n",
      "Test: [200/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 17.49)\tAcc@5   0.00 ( 87.45)\n",
      "Test: [250/352]\tTime  0.053 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 14.01)\tAcc@5   0.00 ( 70.03)\n",
      "Test: [300/352]\tTime  0.054 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 11.68)\tAcc@5   0.00 ( 58.40)\n",
      "Test: [350/352]\tTime  0.052 ( 0.040)\tLoss nan (nan)\tAcc@1   0.00 ( 10.02)\tAcc@5   0.00 ( 50.08)\n",
      " * Acc@1 10.000 Acc@5 50.000\n",
      "lr: [0.09490137878803079]\n",
      "Epoch: [29][  0/352]\tTime  0.253 ( 0.253)\tData  0.159 ( 0.159)\tLoss nan (nan)\tAcc@1   6.64 (  6.64)\tAcc@5  50.78 ( 50.78)\n",
      "Epoch: [29][ 50/352]\tTime  0.072 ( 0.077)\tData  0.000 ( 0.003)\tLoss nan (nan)\tAcc@1  14.06 (  9.70)\tAcc@5  55.86 ( 50.21)\n",
      "Epoch: [29][100/352]\tTime  0.094 ( 0.075)\tData  0.000 ( 0.002)\tLoss nan (nan)\tAcc@1   8.98 (  9.78)\tAcc@5  46.48 ( 50.12)\n",
      "Epoch: [29][150/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1  12.11 ( 10.05)\tAcc@5  48.83 ( 50.04)\n",
      "Epoch: [29][200/352]\tTime  0.072 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   8.20 ( 10.00)\tAcc@5  43.36 ( 49.94)\n",
      "Epoch: [29][250/352]\tTime  0.077 ( 0.074)\tData  0.000 ( 0.001)\tLoss nan (nan)\tAcc@1   9.77 ( 10.00)\tAcc@5  49.22 ( 49.80)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(START_EPOCH, EPOCHS):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#    adjust_learning_rate(optimizer, epoch)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# train for one epoch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# evaluate on validation set\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     acc1 \u001b[38;5;241m=\u001b[39m validate(val_loader, model, criterion)\n",
      "Cell \u001b[0;32mIn[68], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# compute gradient and do SGD step\u001b[39;00m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# measure elapsed time\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:484\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    476\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    477\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    482\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    483\u001b[0m     )\n\u001b[0;32m--> 484\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a SummaryWriter to log accuracy, loss, and learning rate during training\n",
    "writer = SummaryWriter(log_dir=\"/data/runs/\")\n",
    "\n",
    "for epoch in range(START_EPOCH, EPOCHS):\n",
    "#    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    acc1 = validate(val_loader, model, criterion)\n",
    "\n",
    "    # remember best acc@1 and save checkpoint\n",
    "    is_best = acc1 > best_acc1\n",
    "    best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': ARCH,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best)\n",
    "    \n",
    "    scheduler.step()\n",
    "    print('lr: ' + str(scheduler.get_last_lr()))\n",
    "    \n",
    "# Close the SummaryWriter\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce250eb9",
   "metadata": {
    "id": "adc68068"
   },
   "outputs": [],
   "source": [
    "# the logdir should match the parameter passed to what you pass to SummaryWriter, e.g.\n",
    "# writer = SummaryWriter(log_dir=\"/data/runs/\")\n",
    "!tensorboard --logdir=/data/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc94b76",
   "metadata": {
    "id": "d3faf0cd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "cinic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
